input {
	kafka {
    bootstrap_servers => "kafka1:9092"
    client_id => "logstash"
    group_id => "logstash"
    consumer_threads => 1
    topics => ["log"]
    codec => "json"
    tags => ["log", "kafka_source"]
    type => "log"
	request_timeout_ms => "405000"
	session_timeout_ms => "10000"
	max_poll_records => "30"
  }
  # add data file
  file {
    path => "/usr/share/logstash/file/network_traffic_data.json"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => json {
        charset => "ISO-8859-1"
    }
    tags => ["file"] 
  }
}

## Add your filters / logstash plugins configuration here
filter {
  if [type] == "apache_access" {
    grok {
      match => { "message" => "%{COMMONAPACHELOG}" }
    }
    date {
      match => ["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]
      remove_field => ["timestamp"]
    }
  }
  if [type] == "apache_error" {
    grok {
      match => { "message" => "%{COMMONAPACHELOG}" }
    }
    date {
      match => ["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]
      remove_field => ["timestamp"]
    }
  }
}

output {
    if "kafka_source" in [tags] {
		elasticsearch {
			hosts => ["elasticsearch:9200"]
			index => "logstash-%{[type]}-%{+YYYY.MM.dd}"
			user => "elastic"
			password => "changeme"
		}
    }
    if "file" in [tags] {
		elasticsearch {
			hosts => ["elasticsearch:9200"]
			index => "bigginsight"
			user => "elastic"
			password => "changeme"
		}
    }
	stdout { codec => rubydebug }
}
